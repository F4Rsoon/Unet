{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow keras scikit-learn pandas pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHwif0wF1iiU",
        "outputId": "7bdd7f3e-16f5-4aeb-e3c0-89e66bc3c6b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Import libraries\n",
        "# ------------------------------\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# ------------------------------\n",
        "# Parameters and paths\n",
        "# ------------------------------\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "IMG_CHANNELS = 1\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/obj/salt'\n",
        "TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
        "TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
        "TRAIN_CSV = os.path.join(BASE_PATH, 'train.csv')\n",
        "DEPTHS_CSV = os.path.join(BASE_PATH, 'depths.csv')\n",
        "SAMPLE_SUBMISSION_CSV = os.path.join(BASE_PATH, 'sample_submission.csv')\n",
        "\n",
        "# ------------------------------\n",
        "# RLE functions\n",
        "# ------------------------------\n",
        "def rle_decode(mask_rle, shape=(101,101)):\n",
        "    if mask_rle == '' or pd.isna(mask_rle):\n",
        "        return np.zeros(shape)\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape).T\n",
        "\n",
        "def rle_encode(img):\n",
        "    pixels = img.T.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return \" \".join(str(x) for x in runs)\n",
        "\n",
        "# ------------------------------\n",
        "# Dice metric\n",
        "# ------------------------------\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "# ------------------------------\n",
        "# Data loading\n",
        "# ------------------------------\n",
        "def load_and_preprocess_data():\n",
        "    train_df = pd.read_csv(TRAIN_CSV)\n",
        "    depths_df = pd.read_csv(DEPTHS_CSV)\n",
        "    train_df = train_df.merge(depths_df, on='id')\n",
        "\n",
        "    images, masks = [], []\n",
        "    for _, row in train_df.iterrows():\n",
        "        img_path = os.path.join(TRAIN_PATH, 'images', row['id'] + '.png')\n",
        "        img = load_img(img_path, color_mode='grayscale', target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "        img = img_to_array(img) / 255.0\n",
        "        images.append(img)\n",
        "\n",
        "        mask = rle_decode(row['rle_mask'])\n",
        "        mask = Image.fromarray(mask).resize((IMG_HEIGHT, IMG_WIDTH))\n",
        "        mask = img_to_array(mask) / 255.0\n",
        "        masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "def load_test_data():\n",
        "    test_df = pd.read_csv(SAMPLE_SUBMISSION_CSV)\n",
        "    test_images, test_ids = [], []\n",
        "    for _, row in test_df.iterrows():\n",
        "        img_path = os.path.join(TEST_PATH, 'images', row['id'] + '.png')\n",
        "        img = load_img(img_path, color_mode='grayscale', target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "        img = img_to_array(img) / 255.0\n",
        "        test_images.append(img)\n",
        "        test_ids.append(row['id'])\n",
        "    return np.array(test_images), test_ids\n",
        "\n",
        "# ------------------------------\n",
        "# Data generator\n",
        "# ------------------------------\n",
        "def data_generator(images, masks, batch_size=16, augment=True):\n",
        "    num_samples = len(images)\n",
        "    while True:\n",
        "        indices = np.random.permutation(num_samples)\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[i:i+batch_size]\n",
        "            batch_images = images[batch_indices]\n",
        "            batch_masks = masks[batch_indices]\n",
        "\n",
        "            if augment:\n",
        "                for j in range(len(batch_images)):\n",
        "                    if np.random.rand() > 0.5:\n",
        "                        batch_images[j] = np.fliplr(batch_images[j])\n",
        "                        batch_masks[j] = np.fliplr(batch_masks[j])\n",
        "                    if np.random.rand() > 0.5:\n",
        "                        factor = 0.5 + np.random.rand()\n",
        "                        batch_images[j] = np.clip(batch_images[j] * factor, 0, 1)\n",
        "            yield batch_images, batch_masks\n",
        "\n",
        "# ------------------------------\n",
        "# U-Net model\n",
        "# ------------------------------\n",
        "def unet_model():\n",
        "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "\n",
        "    # Contracting path\n",
        "    c1 = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(inputs)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    c1 = Activation('relu')(c1)\n",
        "    c1 = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(c1)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    c1 = Activation('relu')(c1)\n",
        "    p1 = MaxPooling2D((2,2))(c1)\n",
        "    p1 = Dropout(0.1)(p1)\n",
        "\n",
        "    c2 = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(p1)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    c2 = Activation('relu')(c2)\n",
        "    c2 = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(c2)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    c2 = Activation('relu')(c2)\n",
        "    p2 = MaxPooling2D((2,2))(c2)\n",
        "    p2 = Dropout(0.1)(p2)\n",
        "\n",
        "    c3 = Conv2D(256, (3,3), padding='same', kernel_initializer='he_normal')(p2)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "    c3 = Activation('relu')(c3)\n",
        "    c3 = Conv2D(256, (3,3), padding='same', kernel_initializer='he_normal')(c3)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "    c3 = Activation('relu')(c3)\n",
        "    p3 = MaxPooling2D((2,2))(c3)\n",
        "    p3 = Dropout(0.2)(p3)\n",
        "\n",
        "    c4 = Conv2D(512, (3,3), padding='same', kernel_initializer='he_normal')(p3)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "    c4 = Activation('relu')(c4)\n",
        "    c4 = Conv2D(512, (3,3), padding='same', kernel_initializer='he_normal')(c4)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "    c4 = Activation('relu')(c4)\n",
        "    p4 = MaxPooling2D((2,2))(c4)\n",
        "    p4 = Dropout(0.2)(p4)\n",
        "\n",
        "    # Bottleneck\n",
        "    c5 = Conv2D(1024, (3,3), padding='same', kernel_initializer='he_normal')(p4)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "    c5 = Activation('relu')(c5)\n",
        "    c5 = Conv2D(1024, (3,3), padding='same', kernel_initializer='he_normal')(c5)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "    c5 = Activation('relu')(c5)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "\n",
        "    # Expansive path\n",
        "    u6 = UpSampling2D((2,2))(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(512, (3,3), padding='same', kernel_initializer='he_normal')(u6)\n",
        "    c6 = BatchNormalization()(c6)\n",
        "    c6 = Activation('relu')(c6)\n",
        "\n",
        "    u7 = UpSampling2D((2,2))(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(256, (3,3), padding='same', kernel_initializer='he_normal')(u7)\n",
        "    c7 = BatchNormalization()(c7)\n",
        "    c7 = Activation('relu')(c7)\n",
        "\n",
        "    u8 = UpSampling2D((2,2))(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(u8)\n",
        "    c8 = BatchNormalization()(c8)\n",
        "    c8 = Activation('relu')(c8)\n",
        "\n",
        "    u9 = UpSampling2D((2,2))(c8)\n",
        "    u9 = concatenate([u9, c1])\n",
        "    c9 = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(u9)\n",
        "    c9 = BatchNormalization()(c9)\n",
        "    c9 = Activation('relu')(c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1,1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=Adam(1e-4), loss=dice_loss, metrics=['accuracy', dice_coef])\n",
        "    return model\n",
        "\n",
        "# ------------------------------\n",
        "# Training\n",
        "# ------------------------------\n",
        "X, y = load_and_preprocess_data()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = unet_model()\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=15, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.2, patience=10, min_lr=1e-7),\n",
        "    ModelCheckpoint('salt_model_best.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    data_generator(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=len(X_train)//BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# Segmentation (Prediction)\n",
        "# ------------------------------\n",
        "model = load_model(\"salt_model_best.h5\", custom_objects={\"dice_coef\": dice_coef, \"dice_loss\": dice_loss})\n",
        "test_images, test_ids = load_test_data()\n",
        "preds = model.predict(test_images, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "# Threshold to binary masks\n",
        "pred_masks = (preds > 0.5).astype(np.uint8)\n",
        "\n",
        "# ------------------------------\n",
        "# Submission file\n",
        "# ------------------------------\n",
        "submission = pd.read_csv(SAMPLE_SUBMISSION_CSV)\n",
        "for pred, test_id in zip(pred_masks, test_ids):\n",
        "    pred_resized = cv2.resize(np.squeeze(pred), (101, 101))\n",
        "    rle = rle_encode(pred_resized)\n",
        "    submission.loc[submission['id'] == test_id, 'rle_mask'] = rle\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Submission saved: submission.csv\")\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization\n",
        "# ------------------------------\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12,12))\n",
        "for i in range(3):\n",
        "    axes[i,0].imshow(np.squeeze(test_images[i]), cmap=\"gray\")\n",
        "    axes[i,0].set_title(\"Original\")\n",
        "    axes[i,0].axis(\"off\")\n",
        "\n",
        "    axes[i,1].imshow(np.squeeze(preds[i]), cmap=\"jet\")\n",
        "    axes[i,1].set_title(\"Predicted Prob\")\n",
        "    axes[i,1].axis(\"off\")\n",
        "\n",
        "    axes[i,2].imshow(np.squeeze(pred_masks[i]), cmap=\"gray\")\n",
        "    axes[i,2].set_title(\"Segmentation\")\n",
        "    axes[i,2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"segmentation_results.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GdSWyeC8MhE",
        "outputId": "e8718764-d4ad-4fc5-918a-76a6c1b9bef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - accuracy: 0.5312 - dice_coef: 0.0033 - loss: 0.9967"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 441ms/step - accuracy: 0.5323 - dice_coef: 0.0033 - loss: 0.9967 - val_accuracy: 5.2727e-04 - val_dice_coef: 0.0020 - val_loss: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 2/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - accuracy: 0.7207 - dice_coef: 0.0051 - loss: 0.9949"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 482ms/step - accuracy: 0.7207 - dice_coef: 0.0051 - loss: 0.9949 - val_accuracy: 0.0611 - val_dice_coef: 0.0021 - val_loss: 0.9979 - learning_rate: 1.0000e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 0.7271 - dice_coef: 0.0056 - loss: 0.9944"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 470ms/step - accuracy: 0.7272 - dice_coef: 0.0056 - loss: 0.9944 - val_accuracy: 0.4020 - val_dice_coef: 0.0031 - val_loss: 0.9969 - learning_rate: 1.0000e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.7429 - dice_coef: 0.0060 - loss: 0.9940"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 491ms/step - accuracy: 0.7429 - dice_coef: 0.0060 - loss: 0.9940 - val_accuracy: 0.6204 - val_dice_coef: 0.0047 - val_loss: 0.9953 - learning_rate: 1.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.7451 - dice_coef: 0.0064 - loss: 0.9936"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 472ms/step - accuracy: 0.7451 - dice_coef: 0.0064 - loss: 0.9936 - val_accuracy: 0.6975 - val_dice_coef: 0.0057 - val_loss: 0.9943 - learning_rate: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    }
  ]
}